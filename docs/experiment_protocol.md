## Protocole d'expérimentation — optimisation GA (CVRP)

Introduction
------------
Voici le protocole que j'utilise pour évaluer les paramètres du GA et mesurer l'impact du parallélisme. Je l'ai écrit de façon concise pour pouvoir l'exécuter et l'automatiser facilement.

Objectif
--------
Comparer la qualité (coût) et le temps d'exécution des configurations d'algorithme génétique :
- différents jeux de paramètres (population, mutation, crossover, 2-opt),
- différents modes d'exécution (séquentiel, threads, processes).

Sorties
-------
Je produit des fichiers CSV horodatés dans `results/parameter_tests/` et un fichier agrégé `aggregated_results.csv`.

Environnement
-------------
- OS : Windows (PowerShell)
- Python : noter la version avec `python --version`
- Résultats locaux : `results/parameter_tests/` (ignoré par Git)

Données
-------
Je m'assure d'utiliser la même instance VRP (p.ex. `data/data.vrp`) pour toutes les expériences.

Détail de l'instance utilisée
-----------------------------
Je fournis ici les métadonnées et un résumé de l'instance VRP que j'utilise (fichier `data/instances/data.vrp`). Ces informations sont extraites du fichier au format CVRPLIB :

- NAME : X-n153-k22
- COMMENT : Generated by Uchoa, Pecin, Pessoa, Poggi, Subramanian, and Vidal (2013)
- TYPE : CVRP
- DIMENSION : 153 (clients + dépôt)
- EDGE_WEIGHT_TYPE : EUC_2D (distances euclidiennes)
- CAPACITY (capacité véhicule) : 144
- DEPOT_NODE : 1

Résumé chiffré :

- Nombre de nœuds (DIMENSION) : 153
- Demande totale cumulée : 3068
- Véhicules minimum nécessaires (sum(demand) / capacity, arrondi vers le haut) : 22
- Étendue des coordonnées (bounding box) : X in [14, 998], Y in [212, 973]

Ces chiffres permettent de vérifier rapidement que la capacité et le nombre de véhicules sont cohérents pour la campagne d'expérimentation. J'enregistre aussi le nom du fichier et le hash Git dans chaque sortie pour traçabilité.

Schéma des résultats (CSV)
-------------------------
Colonnes que je conserve systématiquement :
- timestamp (ISO)
- config_name
- run_id
- seed
- workers
- mode (sequential|threads|processes)
- cost
- elapsed_seconds
- extra_params (JSON)
- git_hash
- python_version

Design expérimental
--------------------
1) Rédiger `design.csv` listant les configurations à tester. Exemple :

config_name,population_size,mutation_rate,use_2opt,time_limit
pop_small,50,0.01,True,60
pop_large,200,0.02,True,60

2) Pour chaque configuration, lancer N répétitions (N >= 10 recommandé; N=5 acceptable pour pilote).
3) Pour chaque répétition, exécuter au moins threads et processes (ajouter séquentiel si besoin).

Exécution (exemples PowerShell)
-------------------------------
**Tests rapides (validation des tendances) :**
```powershell
# Test ultra-rapide avec multiprocessing
python src/optimization/ultra_quick_test.py --workers 4 --output-dir results/parameter_tests --yes

# Test avec ThreadPoolExecutor (fallback)
python src/optimization/ultra_quick_test.py --workers 2 --output-dir results/parameter_tests
```

**Tests complets (exploration systématique) :**
```powershell  
# Tous les tests (59 indépendants + 5 combinés)
python src/optimization/quick_test.py --workers 4 --output-dir results/parameter_tests

# Test d'une instance spécifique
python src/optimization/quick_test.py --instance data/instances/data.vrp --workers 2
```

**Visualisation des résultats :**
```powershell
# Interface interactive complète
python src/visualization/plot_results.py

# Analyse directe via script
python -c "
import importlib.util
spec = importlib.util.spec_from_file_location('pr', r'src/visualization/plot_results.py')
mod = importlib.util.module_from_spec(spec)
spec.loader.exec_module(mod)
p = mod.ParameterResultsPlotter(r'results/parameter_tests/ultra_quick_results_YYYYMMDD_HHMMSS.txt')
p.print_summary_stats()
"
```

**Nettoyage et maintenance :**
```powershell
# Supprimer les caches Python
Remove-Item -Recurse -Force __pycache__

# Lister les résultats disponibles  
Get-ChildItem results/parameter_tests/ -Name "*.txt" | Sort-Object
```

Agrégation
-----------
Je collecte toutes les sorties individuelles dans `results/parameter_tests/` puis j'alimente `aggregated_results.csv` avec le schéma ci-dessus.

Exemple d'append en Python :

from datetime import datetime
import csv

row = {
  'timestamp': datetime.utcnow().isoformat(),
  'config_name': 'pop_small',
  'run_id': 1,
  'seed': 42,
  'workers': 8,
  'mode': 'threads',
  'cost': 25006.0,
  'elapsed_seconds': 15.1,
  'extra_params': '{"population_size":50,"mutation_rate":0.01}',
  'git_hash': 'abcdef1',
  'python_version': '3.11.4'
}

with open('results/parameter_tests/aggregated_results.csv', 'a', newline='') as f:
  writer = csv.DictWriter(f, fieldnames=list(row.keys()))
  if f.tell() == 0:
    writer.writeheader()
  writer.writerow(row)

Capture de l'environnement
--------------------------
Avant chaque campagne, je note :
- git rev-parse --short HEAD
- python --version
- pip freeze > results/parameter_tests/requirements-YYYYmmdd.txt

Instrumentation système (optionnel)
-----------------------------------
Pour CPU/RAM j'utilise `psutil` et j'échantillonne l'utilisation pendant l'exécution (moyenne/max).

Analyses et visualisation
-----------------------
**Statistiques disponibles :**
- Statistiques par configuration : mean, median, std, min, max pour cost et time.
- **Gap analysis** : Calcul automatique du gap (%) par rapport à la meilleure configuration : `Gap = (Coût - Meilleur) / Meilleur × 100%`
- **Identification des paramètres** : Affichage automatique du paramètre modifié pour chaque test

**Outils de visualisation :**
- Script `src/visualization/plot_results.py` : Interface interactive complète
- Boxplots (coût), barplots (temps moyen), courbes speedup
- Heatmaps de performance, grilles d'histogrammes
- Comparaison meilleure vs pire configuration

**Nouvelles fonctionnalités d'analyse (novembre 2025) :**
1. **Statistiques de synthèse enrichies** :
   - TOP 3 configurations avec gaps calculés
   - Tableau détaillé de TOUTES les configurations classées par performance
   - Identification automatique du paramètre testé pour chaque configuration
   
2. **Format de sortie compatible** :
   - Support des formats `avg_cost`/`min_cost` et `all_costs` 
   - Parsing robuste des fichiers CSV avec fences de code Markdown
   
3. **Exemple de sortie des statistiques de synthèse** :
   ```
   TOP 3 CONFIGURATIONS:
   1. Elitism_Medium - Coût: 23340.5 - Gap vs meilleur: 0.00% - Paramètre modifié: elitism = 6
   2. 2opt_Low - Coût: 23673.5 - Gap vs meilleur: 1.43% - Paramètre modifié: two_opt_prob = 0.2
   3. Mutation_Low - Coût: 23737.0 - Gap vs meilleur: 1.70% - Paramètre modifié: pm = 0.15
   
   TABLEAU DÉTAILLÉ (toutes configurations):
   Rang Configuration        Coût     Gap (%)  Paramètre modifié         Valeur
   1    Elitism_Medium       23340.5  0.00     elitism                   6
   2    2opt_Low             23673.5  1.43     two_opt_prob              0.2
   ...
   ```

**Usage du visualiseur :**
```powershell
python src/visualization/plot_results.py
# Menu interactif pour :
# 1. Statistiques de synthèse (avec gaps et paramètres)
# 2. Impact détaillé par paramètre
# 3. Comparaisons graphiques
# 4-7. Visualisations avancées (heatmaps, etc.)
```

- Tests statistiques (Wilcoxon/t-test) si besoin.

Bonnes pratiques
----------------
- Conserver `design.csv` et scripts d'orchestration dans le dépôt (mais pas `results/`).
- Toujours commencer par un pilote (2–3 configs, N=3).
- Mesurer l'overhead des processes sur Windows avant campagne longue.

Scripts de test disponibles
--------------------------
**1. `ultra_quick_test.py` - Tests indépendants rapides**
- **Stratégie** : 1 seul paramètre modifié à la fois (tests indépendants)  
- **Durée** : 60s par configuration, 2 runs par config
- **Objectif** : Validation rapide des tendances
- **Usage** : `python src/optimization/ultra_quick_test.py --workers 4 --output-dir results/parameter_tests`

**2. `quick_test.py` - Tests systématiques étendus**
- **Stratégie** : Mix de tests indépendants (92%) + combinaisons (8%)
- **Tests indépendants** : 59 configurations (1 paramètre varié)
- **Tests combinés** : 5 configurations prometteuses (6 paramètres simultanés)
- **Durée** : 60s par configuration  
- **Usage** : `python src/optimization/quick_test.py --workers 4`

**Paramètres testés systématiquement :**
- **pop_size** : [40, 60, 80, 100, 120, 140, 160, 180, 200] (9 valeurs)
- **tournament_k** : [2, 3, 4, 5, 6, 7, 8] (7 valeurs)
- **elitism** : [1, 2, 3, 4, 5, 6, 8, 10, 12, 15] (10 valeurs)
- **pc (crossover)** : [0.7, 0.75, 0.8, 0.85, 0.9, 0.92, 0.95, 0.97, 0.98, 0.99] (10 valeurs)
- **pm (mutation)** : [0.05, 0.1, 0.15, 0.2, 0.22, 0.25, 0.27, 0.3, 0.35, 0.4] (10 valeurs)  
- **two_opt_prob** : [0.0, 0.1, 0.2, 0.3, 0.35, 0.4, 0.5, 0.6, 0.65, 0.7, 0.8] (11 valeurs)
- **use_2opt** : [True, False] (2 valeurs)

**Configuration de base (référence) :**
```python
base_config = {
    'pop_size': 100,
    'tournament_k': 4, 
    'elitism': 4,
    'pc': 0.95,
    'pm': 0.25,
    'use_2opt': True,
    'two_opt_prob': 0.35,
    'time_limit': 60.0,
    'generations': 50000
}
```

Automatisation (optionnel)
-------------------------
Je peux ajouter un script `scripts/run_full_experiment.py` qui lit `design.csv`, exécute chaque config N fois pour chaque mode, et écrit `aggregated_results.csv`.

Si vous voulez que je l'écrive, je le créé et lance un pilote pour vérifier.

---

Rédigé par moi — dites-moi si vous voulez que je précise le format exact du CSV, ajoute des exemples concrets de commandes ou que j'implémente l'orchestrateur.


Informations ajoutées (format et exemples)
----------------------------------------

Format des fichiers de résultat (recommandé CSV)
------------------------------------------------
Colonnes recommandées (CSV) :
- timestamp : horodatage ISO (YYYY-MM-DDTHH:MM:SS)
- config_name : nom lisible de la configuration testée
- run_id : identifiant de la répétition (ex. 1..N)
- seed : graine aléatoire utilisée
- workers : nombre de workers utilisés
- mode : "sequential" | "threads" | "processes"
- cost : coût observé pour cette exécution (float)
- elapsed_seconds : temps d'exécution observé (float)
- extra_params : JSON encodé des paramètres non standard (population_size, mutation_rate...)
- git_hash : hash du commit (pour traçabilité)
- python_version : sortie de `python --version`

Exemple de design.csv (en-tête + deux configurations) :

design.csv content:
config_name,population_size,mutation_rate,use_2opt,time_limit
pop_small,50,0.01,True,60
pop_large,200,0.02,True,60

Exemple minimal d'orchestrateur (pseudo-code Python)
---------------------------------------------------
Le but : lire `design.csv`, lancer chaque configuration N fois pour chaque mode et agréger dans un CSV central.

Pseudo-code (description) :
1. Lire design.csv en tant que liste de dicts.
2. Pour chaque configuration et pour run_id in 1..N :
   a. Construire la ligne de commande (ou arguments) pour le script de test. Exemple :
    - python src/optimization/quick_test.py --population_size 50 --mutation_rate 0.01 --time_limit 60 --workers 8 --mode threads
   b. Exécuter le script via subprocess.run() et mesurer la durée (ou laisser le script mesurer et écrire un fichier de sortie).
   c. Récupérer/parse la sortie ou le fichier produit et extraire les champs coût et temps.
   d. Enrichir la ligne avec git_hash et python_version et écrire dans results/parameter_tests/aggregated_results.csv

Remarque : sur Windows, utilisez l'option shell=False et fournissez la commande comme liste d'arguments à subprocess.run().

Commandes utiles pour capturer l'environnement
----------------------------------------------
PowerShell> git rev-parse --short HEAD
PowerShell> python --version
PowerShell> pip freeze > results/parameter_tests/requirements-YYYYmmdd.txt

Instrumentation système (optionnel)
-----------------------------------
- Pour mesurer l'utilisation CPU/RAM par exécution, vous pouvez utiliser la bibliothèque `psutil` et enregistrer la moyenne/max CPU% pendant la durée d'exécution.
- Exemple rapide (dans un script) : surveiller psutil.cpu_percent(interval=0.5) en boucle et stocker la moyenne.

Recommandations de plages de paramètres (points de départ)
----------------------------------------------------------
- population_size : [50, 100, 200]
- mutation_rate : [0.005, 0.01, 0.02]
- crossover_rate : [0.6, 0.8, 1.0]
- use_2opt : [True, False]
- time_limit : [15 (ultra), 60 (long tests)]

Exemple d'append de ligne de résultat en Python (very small snippet) :

from datetime import datetime
import csv

row = {
  'timestamp': datetime.utcnow().isoformat(),
  'config_name': 'pop_small',
  'run_id': 1,
  'seed': 42,
  'workers': 8,
  'mode': 'threads',
  'cost': 25006.0,
  'elapsed_seconds': 15.1,
  'extra_params': '{"population_size":50,"mutation_rate":0.01}',
  'git_hash': 'abcdef1',
  'python_version': '3.11.4'
}

with open('results/parameter_tests/aggregated_results.csv', 'a', newline='') as f:
  writer = csv.DictWriter(f, fieldnames=list(row.keys()))
  if f.tell() == 0:
    writer.writeheader()
  writer.writerow(row)

Conseils pratiques
-------------------
- Toujours faire un pilote réduit (2-3 configurations, N=3) pour valider le pipeline avant une campagne longue.
- Conserver le `design.csv` et les scripts d'orchestration dans le repository (mais pas le dossier `results/`).
- Si vous automatisez, limitez le nombre de processus simultanés pour ne pas saturer la machine et fausser les mesures.

